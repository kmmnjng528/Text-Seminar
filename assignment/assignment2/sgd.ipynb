{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Save parameters every a few SGD iterations as fail-safe\n",
        "SAVE_PARAMS_EVERY = 5000\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as op\n",
        "\n",
        "def load_saved_params():\n",
        "    \"\"\"\n",
        "    A helper function that loads previously saved parameters and resets\n",
        "    iteration start.\n",
        "    \"\"\"\n",
        "    st = 0\n",
        "    for f in glob.glob(\"saved_params_*.npy\"):\n",
        "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
        "        if (iter > st):\n",
        "            st = iter\n",
        "\n",
        "    if st > 0:\n",
        "        params_file = \"saved_params_%d.npy\" % st\n",
        "        state_file = \"saved_state_%d.pickle\" % st\n",
        "        params = np.load(params_file)\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        return st, params, state\n",
        "    else:\n",
        "        return st, None, None\n",
        "\n",
        "\n",
        "def save_params(iter, params):\n",
        "    params_file = \"saved_params_%d.npy\" % iter\n",
        "    np.save(params_file, params)\n",
        "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
        "        pickle.dump(random.getstate(), f)\n",
        "\n",
        "\n",
        "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
        "        PRINT_EVERY=10):\n",
        "    \"\"\" Stochastic Gradient Descent\n",
        "\n",
        "    Implement the stochastic gradient descent method in this function.\n",
        "\n",
        "    Arguments:\n",
        "    f -- the function to optimize, it should take a single\n",
        "         argument and yield two outputs, a loss and the gradient\n",
        "         with respect to the arguments\n",
        "    x0 -- the initial point to start SGD from\n",
        "    step -- the step size for SGD\n",
        "    iterations -- total iterations to run SGD for\n",
        "    postprocessing -- postprocessing function for the parameters\n",
        "                      if necessary. In the case of word2vec we will need to\n",
        "                      normalize the word vectors to have unit length.\n",
        "    PRINT_EVERY -- specifies how many iterations to output loss\n",
        "\n",
        "    Return:\n",
        "    x -- the parameter value after SGD finishes\n",
        "    \"\"\"\n",
        "\n",
        "    # Anneal learning rate every several iterations\n",
        "    ANNEAL_EVERY = 20000\n",
        "\n",
        "    if useSaved:\n",
        "        start_iter, oldx, state = load_saved_params()\n",
        "        if start_iter > 0:\n",
        "            x0 = oldx\n",
        "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
        "\n",
        "        if state:\n",
        "            random.setstate(state)\n",
        "    else:\n",
        "        start_iter = 0\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if not postprocessing:\n",
        "        postprocessing = lambda x: x\n",
        "\n",
        "    exploss = None\n",
        "\n",
        "    for iter in range(start_iter + 1, iterations + 1):\n",
        "        # You might want to print the progress every few iterations.\n",
        "\n",
        "        loss = None\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        x = postprocessing(x)\n",
        "        if iter % PRINT_EVERY == 0:\n",
        "            if not exploss:\n",
        "                exploss = loss\n",
        "            else:\n",
        "                exploss = .95 * exploss + .05 * loss\n",
        "            print(\"iter %d: %f\" % (iter, exploss))\n",
        "\n",
        "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
        "            save_params(iter, x)\n",
        "\n",
        "        if iter % ANNEAL_EVERY == 0:\n",
        "            step *= 0.5\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def sanity_check():\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print(\"Running sanity checks...\")\n",
        "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 1 result:\", t1)\n",
        "    assert abs(t1) <= 1e-6\n",
        "\n",
        "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 2 result:\", t2)\n",
        "    assert abs(t2) <= 1e-6\n",
        "\n",
        "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 3 result:\", t3)\n",
        "    assert abs(t3) <= 1e-6\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sanity_check()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}